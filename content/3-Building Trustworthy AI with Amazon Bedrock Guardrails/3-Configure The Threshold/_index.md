+++
title = "Configure The Threshold"
date = 2020-05-14T00:38:32+07:00
weight = 3
chapter = false
pre = "<b>3.3 </b>"
+++

Configure content filters by adjusting the degree of filtering to detect and block harmful user inputs and model responses that violate your usage policies.

Below are the options that are available:

- **Hate**: Describes input prompts and model responses that discriminate, criticize, insult, denounce, or dehumanize a person or group on the basis of an identity (such as race, ethnicity, gender, religion, sexual orientation, ability, and national origin).

- **Insults**: Describes input prompts and model responses that includes demeaning, humiliating, mocking, insulting, or belittling language. This type of language is also labeled as bullying.

- **Sexual**: Describes input prompts and model responses that indicates sexual interest, activity, or arousal using direct or indirect references to body parts, physical traits, or sex.

- **Violence**: Describes input prompts and model responses that includes glorification of or threats to inflict physical pain, hurt, or injury toward a person, group or thing.

- **Misconduct**: Describes input prompts and model responses that seeks or provides information about engaging in criminal activity, or harming, defrauding, or taking advantage of a person, group or institution.

- **Prompt Attack**: Describes user prompts intended to bypass the safety and moderation capabilities of a foundation model (FM) in order to generate harmful content (also known as jailbreak), and ignore and override instructions specified by the developer (referred to as prompt injection).

For the purposes of this workshop, we will configure Filter Strengths as HIGH
![Threshold](/images/3/Threshold.png?width=90pc)

Once done click on **Next**
